import os
import torch
import numpy as np
import pandas as pd
from PIL import Image
import faiss
from transformers import CLIPProcessor, CLIPModel
from typing import List, Tuple, Union
import pickle
from tqdm import tqdm
import cv2


class CLIPMatcher:
    """åŸºäºCLIPçš„æ–‡æœ¬å’Œå›¾åƒåŒ¹é…ç³»ç»Ÿ"""

    def __init__(self, model_path: str = "../clip-vit-base-patch32", device: str = None):
        """
        åˆå§‹åŒ–CLIPåŒ¹é…å™¨

        Args:
            model_path: CLIPæ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡ ('cuda', 'cpu' æˆ– None è‡ªåŠ¨é€‰æ‹©)
        """
        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åŠ è½½CLIPæ¨¡å‹å’Œå¤„ç†å™¨
        print("æ­£åœ¨åŠ è½½CLIPæ¨¡å‹...")
        self.model = CLIPModel.from_pretrained(model_path)
        self.processor = CLIPProcessor.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()

        # åˆå§‹åŒ–å­˜å‚¨
        self.image_paths = []
        self.image_features = None
        self.text_features = None
        self.captions = []

        # FAISSç´¢å¼•
        self.image_index = None
        self.text_index = None

        print("CLIPåŒ¹é…å™¨åˆå§‹åŒ–å®Œæˆ!")

    def encode_images(self, image_paths: List[str], batch_size: int = 32) -> np.ndarray:
        """
        æ‰¹é‡ç¼–ç å›¾åƒ

        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°

        Returns:
            å›¾åƒç‰¹å¾å‘é‡æ•°ç»„
        """
        features = []

        print(f"æ­£åœ¨ç¼–ç  {len(image_paths)} å¼ å›¾åƒ...")
        for i in tqdm(range(0, len(image_paths), batch_size)):
            batch_paths = image_paths[i:i + batch_size]
            batch_images = []

            for path in batch_paths:
                try:
                    image = Image.open(path).convert('RGB')
                    batch_images.append(image)
                except Exception as e:
                    print(f"æ— æ³•åŠ è½½å›¾åƒ {path}: {e}")
                    # åˆ›å»ºä¸€ä¸ªç©ºç™½å›¾åƒä½œä¸ºå ä½ç¬¦
                    batch_images.append(Image.new('RGB', (224, 224), color='white'))

            if batch_images:
                with torch.no_grad():
                    inputs = self.processor(images=batch_images, return_tensors="pt", padding=True)
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                    image_features = self.model.get_image_features(**inputs)
                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                    features.append(image_features.cpu().numpy())

        return np.vstack(features) if features else np.array([])

    def encode_texts(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """
        æ‰¹é‡ç¼–ç æ–‡æœ¬

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°

        Returns:
            æ–‡æœ¬ç‰¹å¾å‘é‡æ•°ç»„
        """
        features = []

        print(f"æ­£åœ¨ç¼–ç  {len(texts)} æ¡æ–‡æœ¬...")
        for i in tqdm(range(0, len(texts), batch_size)):
            batch_texts = texts[i:i + batch_size]

            with torch.no_grad():
                inputs = self.processor(text=batch_texts, return_tensors="pt", padding=True, truncation=True)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                text_features = self.model.get_text_features(**inputs)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
                features.append(text_features.cpu().numpy())

        return np.vstack(features) if features else np.array([])

    def build_image_index(self, image_dir: str, save_path: str = "image_index.pkl"):
        """
        æ„å»ºå›¾åƒç´¢å¼•

        Args:
            image_dir: å›¾åƒç›®å½•è·¯å¾„
            save_path: ç´¢å¼•ä¿å­˜è·¯å¾„
        """
        # è·å–æ‰€æœ‰å›¾åƒè·¯å¾„
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        self.image_paths = []

        for filename in os.listdir(image_dir):
            if any(filename.lower().endswith(ext) for ext in image_extensions):
                self.image_paths.append(os.path.join(image_dir, filename))

        print(f"æ‰¾åˆ° {len(self.image_paths)} å¼ å›¾åƒ")

        # ç¼–ç å›¾åƒ
        self.image_features = self.encode_images(self.image_paths)

        # æ„å»ºFAISSç´¢å¼•
        if len(self.image_features) > 0:
            dimension = self.image_features.shape[1]
            self.image_index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
            self.image_index.add(self.image_features.astype('float32'))

            # ä¿å­˜ç´¢å¼•
            index_data = {
                'image_paths': self.image_paths,
                'image_features': self.image_features,
                'image_index': faiss.serialize_index(self.image_index)
            }

            with open(save_path, 'wb') as f:
                pickle.dump(index_data, f)

            print(f"å›¾åƒç´¢å¼•å·²ä¿å­˜åˆ° {save_path}")

    def build_text_index(self, captions_file: str, save_path: str = "text_index.pkl"):
        """
        æ„å»ºæ–‡æœ¬ç´¢å¼•

        Args:
            captions_file: å›¾åƒæè¿°æ–‡ä»¶è·¯å¾„
            save_path: ç´¢å¼•ä¿å­˜è·¯å¾„
        """
        # è¯»å–å›¾åƒæè¿°
        try:
            df = pd.read_csv(captions_file)
            self.captions = df['caption'].tolist()
            caption_images = df['image'].tolist()
        except Exception as e:
            print(f"è¯»å–æè¿°æ–‡ä»¶å¤±è´¥: {e}")
            return

        print(f"æ‰¾åˆ° {len(self.captions)} æ¡å›¾åƒæè¿°")

        # ç¼–ç æ–‡æœ¬
        self.text_features = self.encode_texts(self.captions)

        # æ„å»ºFAISSç´¢å¼•
        if len(self.text_features) > 0:
            dimension = self.text_features.shape[1]
            self.text_index = faiss.IndexFlatIP(dimension)
            self.text_index.add(self.text_features.astype('float32'))

            # ä¿å­˜ç´¢å¼•
            index_data = {
                'captions': self.captions,
                'caption_images': caption_images,
                'text_features': self.text_features,
                'text_index': faiss.serialize_index(self.text_index)
            }

            with open(save_path, 'wb') as f:
                pickle.dump(index_data, f)

            print(f"æ–‡æœ¬ç´¢å¼•å·²ä¿å­˜åˆ° {save_path}")

    def load_image_index(self, index_path: str = "image_index.pkl"):
        """åŠ è½½å›¾åƒç´¢å¼•"""
        try:
            with open(index_path, 'rb') as f:
                index_data = pickle.load(f)

            self.image_paths = index_data['image_paths']
            self.image_features = index_data['image_features']
            self.image_index = faiss.deserialize_index(index_data['image_index'])

            print(f"å·²åŠ è½½å›¾åƒç´¢å¼•ï¼ŒåŒ…å« {len(self.image_paths)} å¼ å›¾åƒ")
            return True
        except Exception as e:
            print(f"åŠ è½½å›¾åƒç´¢å¼•å¤±è´¥: {e}")
            return False

    def load_text_index(self, index_path: str = "text_index.pkl"):
        """åŠ è½½æ–‡æœ¬ç´¢å¼•"""
        try:
            with open(index_path, 'rb') as f:
                index_data = pickle.load(f)

            self.captions = index_data['captions']
            self.caption_images = index_data['caption_images']
            self.text_features = index_data['text_features']
            self.text_index = faiss.deserialize_index(index_data['text_index'])

            print(f"å·²åŠ è½½æ–‡æœ¬ç´¢å¼•ï¼ŒåŒ…å« {len(self.captions)} æ¡æè¿°")
            return True
        except Exception as e:
            print(f"åŠ è½½æ–‡æœ¬ç´¢å¼•å¤±è´¥: {e}")
            return False

    # ğŸš¨ NEW FUNCTION: åŸºäºå‘é‡çš„ç›´æ¥æœç´¢æ¥å£ (æ”¯æŒç”¨æˆ·å…´è¶£å‘é‡)
    def search_images_by_vector(self, query_vector: np.ndarray, top_k: int = 5) -> List[Tuple[str, float]]:
        """
        æ ¹æ® CLIP ç‰¹å¾å‘é‡æœç´¢ç›¸ä¼¼å›¾åƒ (æ”¯æŒç”¨æˆ·å…´è¶£å‘é‡)

        Args:
            query_vector: å½’ä¸€åŒ–åçš„ CLIP ç‰¹å¾å‘é‡ (NumPy æ•°ç»„)
            top_k: è¿”å›å‰kä¸ªç»“æœ

        Returns:
            (å›¾åƒè·¯å¾„, ç›¸ä¼¼åº¦åˆ†æ•°) çš„åˆ—è¡¨
        """
        if self.image_index is None:
            return []

        # ç¡®ä¿å‘é‡æ˜¯ float32 ç±»å‹ï¼Œå¹¶ç¡®ä¿å…¶å½¢çŠ¶ä¸º (1, dimension)
        if query_vector.ndim == 1:
            query_vector = query_vector.reshape(1, -1)

        query_vector = query_vector.astype('float32')

        # æœç´¢
        scores, indices = self.image_index.search(query_vector, top_k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.image_paths):
                results.append((self.image_paths[idx], float(score)))

        return results

    def search_images_by_text(self, query_text: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """
        æ ¹æ®æ–‡æœ¬æœç´¢ç›¸ä¼¼å›¾åƒ
        """
        if self.image_index is None:
            return []

        # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        query_features = self.encode_texts([query_text])

        # æœç´¢
        scores, indices = self.image_index.search(query_features.astype('float32'), top_k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.image_paths):
                results.append((self.image_paths[idx], float(score)))

        return results

    def search_images_by_image(self, query_image_path: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """
        æ ¹æ®å›¾åƒæœç´¢ç›¸ä¼¼å›¾åƒ
        """
        if self.image_index is None:
            return []

        # ç¼–ç æŸ¥è¯¢å›¾åƒ
        query_features = self.encode_images([query_image_path])

        # æœç´¢
        scores, indices = self.image_index.search(query_features.astype('float32'), top_k + 1)  # +1 å› ä¸ºå¯èƒ½åŒ…å«è‡ªå·±

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.image_paths):
                image_path = self.image_paths[idx]
                # è·³è¿‡æŸ¥è¯¢å›¾åƒæœ¬èº«
                if os.path.abspath(image_path) != os.path.abspath(query_image_path):
                    results.append((image_path, float(score)))
                    if len(results) >= top_k:
                        break

        return results

    def describe_image(self, image_path: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """
        æè¿°å›¾åƒï¼ˆæ‰¾åˆ°æœ€ç›¸ä¼¼çš„æ–‡æœ¬æè¿°ï¼‰
        """
        if self.text_index is None:
            return []

        # ç¼–ç æŸ¥è¯¢å›¾åƒ
        query_features = self.encode_images([image_path])

        # æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æœ¬
        scores, indices = self.text_index.search(query_features.astype('float32'), top_k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.captions):
                results.append((self.captions[idx], float(score)))

        return results